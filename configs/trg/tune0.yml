# Electron HLT trigger tune0.yml
#
rootname: 'trg'

MAXEVENTS: 10000000      # Maximum total number of events (per root file) to process

eval_reweight: false     # Event-by-event re-weighting used in the evaluation phase (e.g. in ROCs will impact differential shapes, not fractions)


# ----------------------------------------------------
# ** Activate models here **
# Give all models some unique identifier and label
active_models:
  
  - xgb0
  - xgb1

  - cut0
  - cut1
  - cut2
  - cut3
  - cut4
  - cut5
  - cut6

  - cutset0
#  - lgr0
# ----------------------------------------------------

# ----------------------
mcfile:    'eff.root'
datafile:  'datadist.root'
# ----------------------

rngseed: 123456                  # Fixed seed for training data mixing
inputvar: 'MVA_ID'               # Input variables, implemented under mvavars.py


#targetfunc: 'target_e'          # Training target,    implemented under mctargets.py
filterfunc: 'filter_standard'    # Training filtering, implemented under mcfilter.py
cutfunc:    'cut_fiducial'       # Basic cuts,         implemented under cuts.py


xcorr_flow: True                 # Full N-point correlations computed between cuts

frac: 0.8                        # Train/validate/test split fraction
varnorm: 'zscore'                # Variable normalization: 'zscore', 'madscore', 'none'
#varnorm_tensor: 'zscore'        # Tensor variable normalization
#varnorm_graph: 'none'           # Not implemented yet


## Reweighting setup in the training phase
reweight_param:
  
  equal_frac: true              # Equalize integrated class fractions

  # ---------------------
  
  differential_reweight: false
  reference_class: 0            # Reference class: 0 = (background), 1 = (signal), 2 = (another class) ..., 
  # Note about asymmetry: Keep the reference class as 0 == background, because signal
  # MC has much longer tail over pt. Thus, it is not possible to re-weight background
  # to signal very easily, but it is possible to re-weight signal to background.
  # This is seen by inspecting the re-weighted distributions with small and large statistics.
  
  maxevents: 1000000            # Maximum number of events for the PDF construction
  
  var_A: 'x_hlt_pt'
  var_B: 'x_hlt_eta'

  bins_A:  [0.0, 100.0, 200]    # Make sure the bounds cover the phase space
  binmode_A:  'linear'          # 'log10' or 'linear' binning
  
  bins_B: [-3.1, 3.1, 100]
  binmode_B: 'linear'
  
  # ! Variable, and binning min/max boundaries are both transformed !
  transform_A: 'log10'          # 'log10', 'sqrt', 'square', null
  transform_B: null

  max_reg: 10000.0              # Maximum weight cut-off regularization



# Imputation
imputation_param:
  active: true               # True / False
  var: 'MVA_ID'              # Array of variables to be imputated
  algorithm: 'constant'      # Algorithm type: 'constant', iterative' (vector), knn' (vector), 'mean' (scalar), 'median' (scalar)
  fill_value: 0              # For constant imputation
  knn_k: 8                   # Number of nearest neighbours considered
  values: []                 # Special values which indicate the need for imputation
                             # if set empty [], only inf/nan are imputed


## Outlier protection in the training phase
outlier_param:
  algo: 'truncate'   # algorithm: 'truncate', 'none'
  qmin: 0.01         # in [0,100] 
  qmax: 99.9         # in [0,100]


## Pure plotting setup
plot_param:
  basic_on:    false
  contours_on: false


  ## Binned ROC plots can be 1D or 2D;
  # use a running number __0, __1, ... as an identifier for new plots
  plot_ROC_binned__0:
    var:   ['x_hlt_pt']
    edges: [4.0, 6, 8.0, 10.0, 12.0, 15.0, 10000]

  plot_ROC_binned__1:
    var:   ['x_hlt_eta', 'x_hlt_pt']
    edges: [[-1.5, -1.15, -0.75, 0.0, 0.75, 1.15, 1.5],
            [4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 15.0, 10000]]


# ========================================================================
## Raytune setup
# ========================================================================

raytune_param:
  active:         True
  num_samples:    100
  max_num_epochs: 50


raytune_setup:

  xgb_trial_0:
    search_algo: 'HyperOpt'

    search_metric:
      metric: 'AUC'
      mode: 'max'

    param:
      
      ## Booster parameters
      num_boost_round:
        type: "tune.randint"
        value: [1, 100]

      learning_rate:
        type: "tune.uniform"
        value: [0.01, 0.5]
      
      ## Tree parameters
      max_depth:
        type:  "tune.choice"
        value: [4,5,6,7,8,9,10,11,12,13,14,15,16,17]

      ## Regularization
      reg_lambda:
        type: "tune.uniform"
        value: [0.0, 5.0]

      reg_alpha:
        type: "tune.uniform"
        value: [0.0, 5.0]
      
      #min_child_weight: 1.0
      #gamma: 0.0
      #max_delta_step: 0
      #subsample: 0.5

      #colsample_bytree:  1
      #colsample_bylevel: 1
      #colsample_bynode:  1



# ========================================================================
## Classifier setup
# ========================================================================

cutset0_param:
  train:   'cutset'
  predict: 'cutset'

  label:   'cutset'
  raytune:  null
  
  # Using yaml multiline syntax with > character
  cutstring: >
    x_hlt_pms2 < 10000 AND 
    x_hlt_invEInvP < 0.2 AND 
    x_hlt_trkDEtaSeed < 0.01 AND 
    x_hlt_trkDPhi < 0.2 AND 
    x_hlt_trkChi2 < 40 AND 
    x_hlt_trkValidHits >= 5 AND 
    x_hlt_trkNrLayerIT >= 2


cut0_param:
  train:    'cut'
  predict:  'cut'
  label:    'hlt_pms2 x (-1)'
  variable: 'x_hlt_pms2'
  sign: -1
  transform: null

cut1_param:
  train:    'cut'
  predict:  'cut'
  label:    'hlt_invEInvP x (-1)'
  variable: 'x_hlt_invEInvP'
  sign: -1
  transform: 'tanh'

cut2_param:
  train:    'cut'
  predict:  'cut'
  label:    'hlt_trkDEtaSeed x (-1)'
  variable: 'x_hlt_trkDEtaSeed'
  sign: -1
  transform: 'tanh'

cut3_param:
  train:    'cut'
  predict:  'cut'
  label:    'hlt_trkDPhi x (-1)'
  variable: 'x_hlt_trkDPhi'
  sign: -1
  transform: 'tanh'

cut4_param:
  train:    'cut'
  predict:  'cut'
  label:    'hlt_trkChi2 x (-1)'
  variable: 'x_hlt_trkChi2'
  sign: -1
  transform: 'tanh'

cut5_param:
  train:    'cut'
  predict:  'cut'
  label:    'hlt_trkValidHits'
  variable: 'x_hlt_trkValidHits'
  sign: 1
  transform: null

cut6_param:
  train:    'cut'
  predict:  'cut'
  label:    'hlt_trkNrLayerIT'
  variable: 'x_hlt_trkNrLayerIT'
  sign: 1
  transform: null


# XGBoost
# https://xgboost.readthedocs.io/en/latest/parameter.html
xgb0_param:
  train:   'xgb'
  predict: 'xgb'
  label:   'XGB_raytune'
  raytune:  xgb_trial_0

  # general parameters
  booster: 'gbtree'       # 'gbtree' (default), 'dart' (dropout boosting)
  tree_method: 'auto'     # 'auto', 'hist' (CPU), 'gpu_hist' (GPU)
  num_boost_round: 30     # number of epochs (equal to the number of trees!)
  
  # booster parameters
  learning_rate: 0.3
  gamma: 0.0
  max_depth: 6
  min_child_weight: 1.0
  max_delta_step: 1
  subsample: 1

  colsample_bytree:  1
  colsample_bylevel: 1
  colsample_bynode:  1
  
  reg_lambda: 1.0       # L2 regularization
  reg_alpha: 0.0        # L1 regularization
  
  scale_pos_weight: 1

  # learning task parameters
  objective: 'binary:logistic'     #
  eval_metric: ['auc', 'logloss']  # for evaluation

  # Read/Write of epochs
  savemode: 'all'              # 'all', 'latest'
  readmode: -1                 # -1 is the last saved epoch


# XGBoost
# https://xgboost.readthedocs.io/en/latest/parameter.html
xgb1_param:
  train:   'xgb'
  predict: 'xgb'
  label:   'XGB_T1_D6'
  raytune:  null

  # general parameters
  booster: 'gbtree'       # 'gbtree' (default), 'dart' (dropout boosting)
  tree_method: 'auto'     # 'auto', 'hist' (CPU), 'gpu_hist' (GPU)
  num_boost_round: 1      # number of epochs (equal to the number of trees!)
  
  # booster parameters
  learning_rate: 0.3
  gamma: 0.0
  max_depth: 6
  min_child_weight: 1.0
  max_delta_step: 1
  subsample: 1

  colsample_bytree:  1
  colsample_bylevel: 1
  colsample_bynode:  1
  
  reg_lambda: 1.0       # L2 regularization
  reg_alpha: 0.0        # L1 regularization
  
  scale_pos_weight: 1

  # learning task parameters
  objective: 'binary:logistic'     #
  eval_metric: ['auc', 'logloss']  # for evaluation

  # Read/Write of epochs
  savemode: 'all'              # 'all', 'latest'
  readmode: -1                 # -1 is the last saved epoch


# Logistic Regression (convex model = global optimum guarantee)
lgr0_param:
  train:   'lgr'
  predict: 'torch_generic'
  label:   'LGR'
  raytune:  null

  opt_param:
    lossfunc: 'cross_entropy' # cross_entropy, focal_entropy
    gamma: 2                  # focal_entropy exponent
    optimizer: 'AdamW'
    noise_reg: 0.0            # Noise regularization

    epochs: 50
    batch_size: 196
    learning_rate: 0.003
    weight_decay: 0.00001       # L2-regularization

  device: 'auto'            # alternative 'cpu:0', 'cuda:0'
  num_workers: 4

  # Read/Write of epochs
  savemode: 'all'              # 'all', 'latest'
  readmode: -1                 # -1 is the last saved epoch

