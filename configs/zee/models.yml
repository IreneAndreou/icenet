## MVA models

# "Stage 1" model
#
# https://xgboost.readthedocs.io/en/latest/parameter.html
xgb3Drw:
  train:   'xgb'
  predict: 'xgb_logistic'
  label:   'XGB3DRW'
  raytune:  xgb_trial_0
  
  # ** Custom set of variables **
  #include_MVA_vars: ['.*']
  #exclude_MVA_vars: ['.*']
  
  # booster parameters
  model_param:
    num_boost_round: 300      # number of epochs (equal to the number of trees!)
    
    booster: 'gbtree'         # 'gbtree' (default), 'dart' (dropout boosting)
    tree_method: 'hist'
    device:      'auto'       # 'auto', 'cpu', 'cuda'
    
    learning_rate: 0.1
    gamma: 1.5
    max_depth: 15
    min_child_weight: 1.0
    max_delta_step: 1.0
    subsample: 1
    
    colsample_bytree:  1.0
    colsample_bylevel: 1.0
    colsample_bynode:  1.0
    
    reg_lambda: 2.0               # L2 regularization
    reg_alpha: 0.05               # L1 regularization
    
    # learning task parameters
    objective: 'custom:binary_cross_entropy' # Note that 'multi:softprob' does not work with distillation
    eval_metric: ['custom']                  # for custom losses, otherwise 'logloss', 'mlogloss' ...
  
  # BCE loss domains
  BCE_param:
    main:
      classes: [0,1]
      beta: 1.0
      #set_filter: *MAIN_DOMAIN_FILTER # Comment out for 'inclusive'
  
  plot_trees: False
  
  # Read/Write of epochs
  evalmode: 25                  # Every n-th epoch (integer), evaluation done also for every n-th
  readmode: -1                  # -1 is the last saved epoch


# ------------------------------------------------
# "Stage 2" models

# Plain XGBoost
# https://xgboost.readthedocs.io/en/latest/parameter.html
xgb0:
  train:   'xgb'
  predict: 'xgb_scalar'
  label:   'XGB'
  raytune:  xgb_setup_0
  
  # ** Custom set of variables **
  #include_MVA_vars: ['.*']
  #exclude_MVA_vars: ['.*']
  
  # booster parameters
  model_param:
    num_boost_round: 500        # number of epochs (equal to the number of trees!)
    
    booster: 'gbtree'           # 'gbtree' (default), 'dart' (dropout boosting)
    tree_method: 'hist'
    device:      'auto'         # 'auto', 'cpu', 'cuda'
    
    learning_rate: 0.1
    gamma: 1.5
    max_depth: 15
    min_child_weight: 1.0
    max_delta_step: 1.0
    subsample: 1

    colsample_bytree:  0.9
    colsample_bylevel: 0.9
    colsample_bynode:  0.9
    
    reg_lambda: 2.0               # L2 regularization
    reg_alpha: 0.05               # L1 regularization
    
    # learning task parameters
    objective: 'binary:logistic'  # Note that 'multi:softprob' does not work with distillation
    eval_metric: ['logloss']      # for custom losses, otherwise 'logloss', 'mlogloss' ...
  
  plot_trees: False
  
  # Read/Write of epochs
  evalmode: 25                    # Every n-th epoch (integer), evaluation done also for every n-th
  readmode: -1                    # -1 is the last saved epoch


# ICEBOOST with custom loss
# 
iceboost0:
  train:   'xgb'
  predict: 'xgb_logistic'
  label:   'ICEBOOST'
  raytune:  xgb_setup_0
  
  # ** Custom set of variables **
  #include_MVA_vars: ['.*']
  #exclude_MVA_vars: ['.*']
  
  # booster parameters
  model_param:
    num_boost_round: 500        # number of epochs (equal to the number of trees!)
    
    booster: 'gbtree'           # 'gbtree' (default), 'dart' (dropout boosting)
    tree_method: 'hist'
    device:      'auto'         # 'auto', 'cpu', 'cuda'
    
    learning_rate: 0.1
    gamma: 1.5
    max_depth: 15
    min_child_weight: 1.0
    max_delta_step: 1.0
    subsample: 1

    colsample_bytree:  0.9
    colsample_bylevel: 0.9
    colsample_bynode:  0.9
    
    reg_lambda: 2.0               # L2 regularization
    reg_alpha: 0.05               # L1 regularization
    
    # learning task parameters
    objective: 'custom:binary_cross_entropy' # Note that 'multi:softprob' does not work with distillation
    eval_metric: ['custom']                  # for custom losses, otherwise 'logloss', 'mlogloss' ...
  
  # BCE loss domains
  BCE_param:
    main:
      classes: [0,1]
      beta: 1.0
      #set_filter: *MAIN_DOMAIN_FILTER # Comment out for 'inclusive'
  
  plot_trees: False
  
  # Read/Write of epochs
  evalmode: 25                  # Every n-th epoch (integer), evaluation done also for every n-th
  readmode: -1                  # -1 is the last saved epoch


# ICEBOOST with an additional re-weighting in-the-loop regularization
# 
iceboost_plus:
  train:   'xgb'
  predict: 'xgb_scalar'
  label:   'ICEBOOST-PLUS'
  raytune:  xgb_setup_0
  
  # ** Custom set of variables **
  #include_MVA_vars: ['.*']
  #exclude_MVA_vars: ['.*']
  
  # booster parameters
  model_param:
    num_boost_round: 500        # number of epochs (equal to the number of trees!)
    
    booster: 'gbtree'           # 'gbtree' (default), 'dart' (dropout boosting)
    tree_method: 'hist'
    device:      'auto'         # 'auto', 'cpu', 'cuda'
    
    learning_rate: 0.1
    gamma: 1.5
    max_depth: 15
    min_child_weight: 1.0
    max_delta_step: 1.0
    subsample: 1

    colsample_bytree:  0.9
    colsample_bylevel: 0.9
    colsample_bynode:  0.9
    
    reg_lambda: 2.0               # L2 regularization
    reg_alpha: 0.05               # L1 regularization
    
    # learning task parameters
    objective: 'custom:binary_cross_entropy' # Note that 'multi:softprob' does not work with distillation
    eval_metric: ['custom']                  # for custom losses, otherwise 'logloss', 'mlogloss' ...
  
  # BCE loss domains
  BCE_param:
    main:
      classes: [0,1]
      beta: 1.0
      
      #set_filter: *MAIN_DOMAIN_FILTER # Comment out for 'inclusive'
    
    flat:
      classes: [0,1]
      beta: 0.01
      AIRW:
        classes:  [0]      # One (or two) classes
        RW_modes: ['LR']   # 'LR', 'inverse-LR', 'DeepEfficiency', 'direct', 'identity'

      #set_filter: *MAIN_DOMAIN_FILTER # Comment out for 'inclusive'
  
  #MI_param:
  #  beta: [0.1]                 # Positive for minimizing (for each class in use)
  #  <<: *MI_REG_PARAM
  
  plot_trees: False

  # Read/Write of epochs
  evalmode: 25                  # Every n-th epoch (integer), evaluation done also for every n-th
  readmode: -1                  # -1 is the last saved epoch

# -----------------------------------------------------------------------------
# Remember to use 'zscore-weighted' (or 'zscore') typically with Neural Networks,
# however, performance with BDTs may be better without.
# -----------------------------------------------------------------------------

## Lipschitz MLP
#
lzmlp0: &LZMLP
  train:   'torch_generic'
  predict: 'torch_scalar'
  raw_logit: true               # Return raw logits

  label:   'LZMLP'
  raytune:  null
  
  # ** Custom set of variables **
  #include_MVA_vars: ['.*']
  #exclude_MVA_vars: ['.*']
  
  # Model
  conv_type: 'lzmlp'
  out_dim: 1   # We want to use sigmoid 1D-output model, comment out for default softmax multiclass

  model_param:
    mlp_dim: [64, 64, 64, 64]         # hidden layer dimensions
    activation: 'silu'
    layer_norm: True
    batch_norm: False                 # normalization layers & dropout can be ill-posed here (operators not 1-to-1 compatible with weighted events)
    dropout: 0
    act_after_norm: True
  
  # Optimization
  opt_param:  
    lossfunc: 'binary_cross_entropy'  # binary_cross_entropy, cross_entropy, focal_entropy, logit_norm_cross_entropy
    lipschitz_beta:    5.0e-5         # lipschitz regularization (use with 'lzmlp')
    #logit_L1_beta: 1.0e-2            # logit norm reg. ~ beta * torch.sum(|logits|)
    logit_L2_beta: 5.0e-3             # logit norm reg. ~ beta * torch.sum(logits**2)

    #gamma: 2                         # focal_entropy "exponent"
    #temperature: 1                   # logit_norm_cross_entropy "temperature"
    
    optimizer: 'AdamW'
    clip_norm: 1.0
    
    epochs: 300
    batch_size: 2048
    lr: 1.0e-4
    weight_decay: 1.0e-4       # L2-regularization
  
  # Scheduler
  scheduler_param:
    step_size: 200             # Number of epochs for drop
    gamma: 0.1
  
  device: 'auto'               # alternative 'cpu:0', 'cuda:0'
  num_workers: 4
  
  # Read/Write of epochs
  evalmode: 10                 # Every n-th epoch (integer), evaluation done also for every n-th
  readmode: -1                 # -1 is the last saved epoch
  
  eval_batch_size: 4096


## Lipschitz MLP
#
lzmlp0_nozero:

  <<: *LZMLP

  # ** Custom set of variables **
  #include_MVA_vars: ['.*']
  exclude_MVA_vars: ['probe_pfChargedIso', 'probe_ecalPFClusterIso', 'probe_trkSumPtHollowConeDR03', 'probe_trkSumPtSolidConeDR04']
  
  label:   'LZMLP-NOZERO'


## FastKAN
#
fastkan0: &FASTKAN
  train:   'torch_generic'
  predict: 'torch_scalar'
  raw_logit: true               # Return raw logits

  label:   'FASTKAN'
  raytune:  null
  
  # ** Custom set of variables **
  #include_MVA_vars: ['.*']
  #exclude_MVA_vars: ['.*']
  
  # Model
  conv_type: 'fastkan'
  out_dim: 1   # We want to use sigmoid 1D-output model, comment out for default softmax multiclass
  
  model_param:
    grid_min: -2.0                    # Activation learning param
    grid_max:  2.0                    # 
    num_grids:   8                    # 
    mlp_dim: [64, 64, 64]             # hidden layer dimensions
    use_base_update: True             # Use "base MLP" in addition
    
    last_tanh: True                   # Extra tanh layer
    last_tanh_scale: 10.0             # Scale after tanh()
  
  # Optimization
  opt_param:  
    lossfunc: 'binary_cross_entropy'  # binary_cross_entropy, cross_entropy, focal_entropy, logit_norm_cross_entropy
    #lipshitz_beta:    1.0e-4         # Lipshitz regularization (use with 'lzmlp')
    #logit_L1_beta: 1.0e-2            # logit norm reg. ~ beta * torch.sum(|logits|)
    logit_L2_beta: 5.0e-3             # logit norm reg. ~ beta * torch.sum(logits**2)

    #gamma: 2                         # focal_entropy "exponent"
    #temperature: 1                   # logit_norm_cross_entropy "temperature"
    
    optimizer: 'AdamW'
    clip_norm: 1.0
    
    epochs: 300
    batch_size: 2048
    lr: 1.0e-4
    weight_decay: 1.0e-4       # L2-regularization
  
  # Scheduler
  scheduler_param:
    step_size: 200             # Number of epochs for drop
    gamma: 0.1
  
  device: 'auto'               # alternative 'cpu:0', 'cuda:0'
  num_workers: 4
  
  # Read/Write of epochs
  evalmode: 10                 # Every n-th epoch (integer), evaluation done also for every n-th
  readmode: -1                 # -1 is the last saved epoch
  
  eval_batch_size: 4096

  # Deploy (or test) mode device
  deploy_device: 'cpu'        # 'auto', 'cpu', 'cuda'


## FastKAN
#
fastkan0_nozero:

  <<: *FASTKAN
  
  # ** Custom set of variables **
  #include_MVA_vars: ['.*']
  exclude_MVA_vars: ['probe_pfChargedIso', 'probe_ecalPFClusterIso', 'probe_trkSumPtHollowConeDR03', 'probe_trkSumPtSolidConeDR04']
  
  label:   'FASTKAN-NOZERO'


## Deep MLP
#
dmlp0: &DMLP
  train:   'torch_generic'
  predict: 'torch_scalar'
  raw_logit: true               # Return raw logits

  label:   'DMLP'
  raytune:  null
  
  # ** Custom set of variables **
  #include_MVA_vars: ['.*']
  #exclude_MVA_vars: ['.*']
  
  # Model
  conv_type: 'dmlp'
  out_dim: 1   # We want to use sigmoid 1D-output model, comment out for default softmax multiclass
  
  model_param:
    mlp_dim: [64, 64, 64, 64]         # hidden layer dimensions
    activation: 'silu'
    layer_norm: True
    batch_norm: False                 # normalization layers & dropout can be ill-posed here (operators not 1-to-1 compatible with weighted events)
    dropout: 0
    act_after_norm: True
    
    skip_connections: False
    last_tanh: True                   # Extra tanh layer
    last_tanh_scale: 10.0             # Scale after tanh()

  # Optimization
  opt_param:  
    lossfunc: 'binary_cross_entropy'  # binary_cross_entropy, cross_entropy, focal_entropy, logit_norm_cross_entropy
    #logit_L1_beta: 1.0e-2            # logit norm reg. ~ lambda * torch.sum(|logits|)
    logit_L2_beta: 5.0e-3             # logit norm reg. ~ lambda * torch.sum(logits**2)

    #gamma: 2                         # focal_entropy "exponent"
    #temperature: 1                   # logit_norm_cross_entropy "temperature"
    
    optimizer: 'AdamW'
    clip_norm: 1.0
    
    epochs: 300
    batch_size: 2048
    lr: 1.0e-4
    weight_decay: 1.0e-4       # L2-regularization
  
  # Scheduler
  scheduler_param:
    step_size: 200             # Number of epochs for drop
    gamma: 0.1
  
  device: 'auto'               # alternative 'cpu:0', 'cuda:0'
  num_workers: 4
  
  # Read/Write of epochs
  evalmode: 10                 # Every n-th epoch (integer), evaluation done also for every n-th
  readmode: -1                 # -1 is the last saved epoch
  
  eval_batch_size: 4096


## Deep MLP
#
dmlp0_nozero:

  <<: *DMLP

  # ** Custom set of variables **
  #include_MVA_vars: ['.*']
  exclude_MVA_vars: ['probe_pfChargedIso', 'probe_ecalPFClusterIso', 'probe_trkSumPtHollowConeDR03', 'probe_trkSumPtSolidConeDR04']
  
  label:   'DMLP-NOZERO'


## Deep Normalizing Flow
#
dbnf0:
  train:   'flow'
  predict: 'torch_flow'
  label:   'DBNF'
  raytune:  null

  # Gradient descent
  opt_param:
    lossfunc: 'flow_logpx'
    optimizer: 'Adam'
    clip_norm: 0.1
    
    epochs: 250
    batch_size: 512           # Keep it high!
    lr: 1.0e-3
    weight_decay: 1.0e-5      # L2-regularization
    polyak: 0.998

    start_epoch: 1
  
  # Learning rate reduction on plateau
  scheduler_param:  
    factor:  0.1
    patience: 20
    cooldown: 10
    min_lr: 0.0005
    early_stopping: 100

  # Model structure
  model_param:  
    flows: 10                 # number of flow blocks
    layers: 0                 # intermediate layers in a flow block
    hidden_dim: 10            # 
    residual: 'gated'         # choises 'none', 'normal', 'gated'
    perm: 'rand'              # flow permutation: choises 'none', 'flip', 'rand'
  
  tensorboard: True
  device: 'auto'              # alternative 'cpu:0', 'cuda:0'
  num_workers: 4

  # Read/Write of epochs
  evalmode: 1                 # Every n-th epoch (integer), evaluation done also for every n-th
  readmode: -1                # -1 is the last saved epoch

  eval_batch_size: 4096
