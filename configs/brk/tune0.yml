# B/RK analyzer config file (PROTO)
#
# -----------------------------------------------------------------------------
# General

rootname: 'brk'

VERBOSE: false
maxevents: 1000

rngseed: 123456                 # Fixed seed for training data mixing

num_classes: null               # Num of classes in MVA
signalclass: null

# -----------------------------------------------------------------------------
# TRAINING AND ANALYSIS
# -----------------------------------------------------------------------------

# Maximum number of triplets as an input to the neural network
MAXT3: 6

# Maximum number of simultaneous "hot" objects considered with 0 < MAXN <= MAXT3
# Note! Exponential complexity growth
MAXN:  1

# Create supersets (combine triplets with identical kinematics)
SUPERSETS: true


# Input imputation (NOT IMPLEMENTED)
# ...

# Variable normalization
frac: 0.9                 # Train/validate/test split fraction
varnorm: 'zscore'         # variable normalization: 'zscore', 'minmax', 'none'

## Outlier protection in the training phase
outlier_param:
  algo: 'truncate' # algorithm: 'truncate', 'none'
  qmin:  0.001   # in [0,100] 
  qmax: 99.99    # in [0,100]


# -----------------------------------------------------------------------------
# ANALYSIS ONLY
# -----------------------------------------------------------------------------

# Normalization
WNORM: 'event'


# -----------------------------------------------------------------------------
# WNORM: event-by-event weight normalization:
#
#   'event' gives ~ #active-elements^{-1} normalized weights,
#           e.g class c3 = [1 0 0 1 0 0 0] (two hot objects) receives
#           weight \hat{P}(c_3) / 2 filled two times by two different kinematics
#
#   'unit'  gives weights normalized by one, the relative weight
#           per output c_i depends on the object multiplicity of the class
# -----------------------------------------------------------------------------


# -----------------------------------------------------------------------------
# Classifier training


#inputvar: ''                 # input variables tag
#rngseed: 123456              # fixed seed for training data mixing
#frac: 0.9                    # train/validate/test split fraction


## Pure plotting setup
#plot_param:
#  basic:
#    active: true
#    nbins:  70
#  
#  contours: 
#    active: false
#  
#  # (eta,pt)-binned plots
#  pt_edges:  [0, 0.75, 1.0, 1.25, 1.5, 1.75, 2.5, 4.0, 10, 10000]
#  eta_edges: [-10, -2.5, -1.5, -0.75, 0.0, 0.75, 1.5, 2.5, 10]


## Reweighting setup in the training phase
#reweight_param:
#  mode: 'background' # 'none', 'signal', 'background'
#  algo: '2D'
#  bins_pt:  [0.0, 300.0, 1000] # Make sure the bounds cover the phase space
#  bins_eta: [-3.1, 3.1,   100] # 
#  max_reg: 50.0                # maxweight regularization


## Classifier setup

# XGBoost
# https://xgboost.readthedocs.io/en/latest/parameter.html
xgb_param:
  active: true

  train:   'xgb'
  predict: 'xgb'
  label:   'XGB'
  raytune: null

  # general parameters
  booster: 'gbtree'      # 'gbtree' (default), 'dart' (dropout boosting)
  num_boost_round: 300   # Number of epochs
  tree_method: 'auto' # 'auto', 'hist' (CPU), 'gpu_hist' (GPU)
  
  # booster parameters
  learning_rate: 0.3
  
  max_depth: 10
  min_child_weight: 1.0
  gamma: 0.0
  max_delta_step: 0
  subsample: 1.0

  colsample_bytree:  1
  colsample_bylevel: 1
  colsample_bynode:  1
  
  reg_lambda: 2.0       # L2 regularization
  reg_alpha: 0.0        # L1 regularization
  
  scale_pos_weight: 1
  
  # learning task parameters
  objective: 'multi:softprob' # 'binary:logitraw'  # 'binary:logistic'
  eval_metric: ['mlogloss', 'auc']   # 'mlogloss' is multiclass log-loss, 'logloss'

  # Read/Write of epochs
  savemode: 'all'              # 'all', 'latest'
  readmode: -1                 # -1 is the last saved epoch


# Deep MaxOut network
maxo_param:
  active: true
  
  train:   'torch_generic'
  predict: 'torch_vector'
  label:  'MAXO'
  raytune:  null

  # Model parameters
  conv_type: 'maxo'
  model_param:
    num_units: 12
    neurons:  36
    dropout:  0.5

  # Optimization
  opt_param:  
    lossfunc: 'cross_entropy' # cross_entropy, focal_entropy, logit_norm_cross_entropy
    gamma: 2                  # focal_entropy exponent
    temperature: 1            # logit norm temperature

    optimizer: 'AdamW'
    noise_reg: 0.0                # Noise regularization
    epochs:  200
    batch_size:  256
    learning_rate: 0.005
    weight_decay: 0.01            # L2-regularization

  # Scheduler
  scheduler_param:
    step_size: 200
    gamma: 0.1
    
  device: 'auto'                # alternative 'cpu:0', 'cuda:0'
  num_workers: 1

  # Read/Write of epochs
  savemode: 'all'              # 'all', 'latest'
  readmode: -1                 # -1 is the last saved epoch


# Permutation Equivariant Network
deps_param:
  active: true
  
  train:   'torch_generic'
  predict: 'torch_vector'
  label:   'DEPS'
  raytune:  null

  # Model parameters
  conv_type: 'deps'
  model_param:  
    z_dim: 64                  # Latent dimension
    pool: 'max'
    dropout: 0.5
    phi_layers: 3
    rho_layers: 3

  # Optimization
  opt_param:  
    lossfunc: 'cross_entropy'  # cross_entropy, focal_entropy, logit_norm_cross_entropy
    gamma: 2                   # focal_entropy exponent
    temperature: 1             # logit norm temperature
    
    optimizer: 'AdamW'
    noise_reg: 0.0             # Noise regularization
    epochs:  200
    batch_size:  256
    learning_rate: 0.005
    weight_decay: 0.01         # L2-regularization

  # Scheduler
  scheduler_param:
    step_size: 200
    gamma: 0.1
    
  device: 'auto'               # alternative 'cpu:0', 'cuda:0'
  num_workers: 1
  
  # Read/Write of epochs
  savemode: 'all'              # 'all', 'latest'
  readmode: -1                 # -1 is the last saved epoch
