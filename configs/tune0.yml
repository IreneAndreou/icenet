# tune0.yml
#
#

inputvar: 'CMSSW_MVA_ID'     # input variables tag
rngseed: 123456              # fixed seed for training data mixing
inputation: true             # missing component inputation
frac: 0.9                    # train/validate/test split fraction
varnorm: 'zscore'            # variable normalization: 'zscore', 'none'


## Pure plotting setup
plot_param:
  basic_on:    true
  contours_on: false

  # (eta,pt)-binned plots
  pt_edges:  [0, 0.75, 1.0, 1.25, 1.5, 1.75, 2.5, 4.0, 10, 10000]
  eta_edges: [-10, -2.5, -1.5, -0.75, 0.0, 0.75, 1.5, 2.5, 10]


## Outlier protection in the training phase
outlier_param:
  algo: 'truncate' # algorithm: 'truncate', 'none'
  qmin:  0.01   # in [0,100] 
  qmax: 99.9    # in [0,100]


## Reweighting setup in the training phase
reweight_param:
  mode: 'background' # 'none', 'signal', 'background'
  algo: '2D'
  bins_pt:  [0.0, 300.0, 1000] # Make sure the bounds cover the phase space
  bins_eta: [-3.1, 3.1,   100] # 
  max_reg: 50.0                # maxweight regularization


## Classifier setup
# -----------------------------------------------

# Factorized Likelihood Ratio
flr_param:
  label: 'FLR'
  active: true

  nbins: 60
  qmin:  0.5 # in [0,100]
  qmax: 99.5 # in [0,100]


# XGBoost
xgb_param:
  label: 'XGB'
  active: true

  # general parameters
  booster: 'gbtree'
  num_boost_round: 50 # Number of epochs


  # booster parameters
  n_estimators: 2000
  learning_rate: 0.1
  gamma: 0.0
  max_depth: 15
  min_child_weight: 1.0
  max_delta_step: 0
  subsample: 1.0
  colsample_bytree:  1
  colsample_bylevel: 1
  colsample_bynode:  1
  reg_lambda: 2.0 # L2 regularization
  reg_alpha: 0  # L1 regularization
  scale_pos_weight: 1
  

  # learning task parameters
  objective: 'binary:logitraw'  # 'binary:logistic'
  eval_metric: ['auc', 'logloss']  # for evaluation


# Logistic Regression (convex algorithm = global optimum guarantee)
lgr_param:
  active: true
  device: 'cpu:0' # alternative 'cuda:0'
  label:  'LGR'

  lossfunc: 'cross_entropy' # cross_entropy, focal_entropy
  gamma: 2 # focal_entropy exponent

  epochs: 10
  batch_size: 256
  learning_rate: 0.003


# XTX classifier
xtx_param:
  active: false
  device: 'cpu:0' # alternative 'cuda:0'
  label:  'XTX'

  lossfunc: 'cross_entropy' # cross_entropy, focal_entropy
  gamma: 3 # focal loss exponent
  
  epochs: 50
  batch_size: 256
  learning_rate: 0.003


# Deep MaxOut-MLP network
dmlp_param:
  active: true
  device: 'cpu:0' # alternative 'cuda:0'
  label:  'DMLP'

  lossfunc: 'focal_entropy' # cross_entropy, focal_entropy
  gamma: 2 # focal_entropy exponent parameter
  
  num_units: 18
  neurons:  36
  dropout:  0.1

  epochs:  100
  batch_size:  512
  learning_rate: 0.003


# Deep Normalizing Flow
dbnf_param:
  active: false
  device: 'cpu:0' # alternative 'cuda:0'
  label:  'DBNF'
  
  epochs: 3
  batch_size: 256
  learning_rate: 0.01
  clip_norm: 0.1

  patience: 20
  cooldown: 10
  early_stopping: 100
  decay: 0.5
  min_lr: 5e-4
  polyak: 0.998

  flows: 5
  layers: 0
  hidden_dim: 8
  residual: 'gated' # choises None, normal, gated

  modelname: 'null'
  start_epoch: 0
  tensorboard: 'tensorboard'

