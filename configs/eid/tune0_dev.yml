# Electron ID tune0.yml
#
# General comments:
#
# - Keep the minibatch size small enough, otherwise weak convergence to optima with some models
# - Other imputation methods than 'constant' not natively compatible with re-weighting (-> biased)

MAXEVENTS: 300000


rngseed: 123456                # Fixed seed for training data mixing
inputvar: 'CMSSW_MVA_ID_ORIG'  # Input variables, implemented under mvavars.py

targetfunc: 'target_e'         # Training target,    implemented under mctargets.py
filterfunc: 'filter_no_egamma' # Training filtering, implemented under mcfilter.py
xcorr_flow: True               # Full N-point correlations computed between cuts


cutfunc: 'cut_standard'        # Basic cuts,         implemented under cuts.py
frac: 0.9                      # Train/validate/test split fraction
varnorm: 'zscore'              # Variable normalization: 'zscore', 'madscore', 'none'
varnorm_tensor: 'zscore'       # Tensor variable normalization
#varnorm_graph: 'none'         # Not implemented yet


# ** Graph object construction **
graph_param:
  global_on: True
  coord: 'pxpypze'             # 'ptetaphim', 'pxpypze'


# ** Image tensor object construction **
image_param:

  # See the corresponding construction under common.py
  channels: 2                 # 1,2,...

  # bin-edges
  eta_bins: [-1.5, -1.4, -1.3, -1.2, -1.1, -1. , -0.9, -0.8, -0.7, -0.6, -0.5,
       -0.4, -0.3, -0.2, -0.1,  0. ,  0.1,  0.2,  0.3,  0.4,  0.5,  0.6,
        0.7,  0.8,  0.9,  1. ,  1.1,  1.2,  1.3,  1.4,  1.5]
  phi_bins: [-3.14159   , -2.93215067, -2.72271133, -2.513272  , -2.30383267,
       -2.09439333, -1.884954  , -1.67551467, -1.46607533, -1.256636  ,
       -1.04719667, -0.83775733, -0.628318  , -0.41887867, -0.20943933,
        0.        ,  0.20943933,  0.41887867,  0.628318  ,  0.83775733,
        1.04719667,  1.256636  ,  1.46607533,  1.67551467,  1.884954  ,
        2.09439333,  2.30383267,  2.513272  ,  2.72271133,  2.93215067,
        3.14159   ]


# Imputation
imputation_param:
  active: true                 # True / False
  var: 'CMSSW_MVA_ID_ORIG'     # Array of variables to be imputated
  algorithm: 'constant'        # Algorithm type: 'constant', iterative' (vector), knn' (vector), 'mean' (scalar), 'median' (scalar)
  fill_value: 0                # For constant imputation
  knn_k: 8                     # Number of nearest neighbours considered
  values: [-999, -666, -10]    # Special values which indicate the need for imputation


## Outlier protection in the training phase
outlier_param:
  algo: 'truncate'   # algorithm: 'truncate', 'none'
  qmin: 0.01         # in [0,100] 
  qmax: 99.9         # in [0,100]


## Reweighting setup in the training phase
reweight_param:
  mode:  'background'           # 'none', 'signal', 'background'
  algo: '2D'
  bins_pt:  [0.0, 300.0, 1000] # Make sure the bounds cover the phase space
  bins_eta: [-3.1, 3.1,   100] # 
  max_reg: 50.0                # maxweight regularization


## Pure plotting setup
plot_param:
  basic_on:    false
  contours_on: false
  
  # (eta,pt)-binned plots
  pt_edges:  [0.5, 0.7, 1.0, 1.25, 1.5, 1.75, 2.5, 4.0, 10, 10000]
  eta_edges: [-2.5, -1.5, -1.15, -0.75, 0.0, 0.75, 1.15, 1.5, 2.5]


# ========================================================================
## Classifier setup
# ========================================================================


# ** Activate models here **
# Give all models some unique identifier and label
active_models:

#  - dmlp
#  - cnn0
#  - gnet0
#  - gnet1
#  - gnet3
#  - gnet2
#  - flr0
  - xgb0
#  - lgr0
#  - xtx0
#  - dmx0
#  - dbnf0
  - gxgb0 # Keep this last (fix the bug) !


# Dual graph + XGB network
gxgb0_param:

  train:   'graph_xgb'
  predict: 'graph_xgb'
  label:   'DGXGB'

  # ** GRAPH NETWORK **
  graph:
    train: 'graph'
    predict: 'torch_graph'
    label:  'GXGB-G'

    # Convolution model
    conv_type: 'SUP'           # 'NN', SAGE', 'GINE', 'DEC', spline', 'SG', 'GAT'
    conv_aggr:   'add'
    global_pool: 'max'
    
    epochs: 20
    batch_size: 64
    learning_rate: 0.003
    weight_decay:  0.00005     # L2-regularization

    # Scheduler
    step_size: 200
    gamma: 0.1

    device: 'auto'            # alternative 'cpu:0', 'cuda:0'
    num_workers: 1

  # ** XGBOOST **
  xgb:
    train: 'xgb'
    predict: 'xgb'
    label: 'GXGB-XGB'
    
    # general parameters
    booster: 'gbtree'       # 'gbtree' (default), 'dart' (dropout boosting)
    num_boost_round: 300    # number of epochs (equal to the number of trees!)
    tree_method: 'auto'     # 'auto', 'hist' (CPU), 'gpu_hist' (GPU)

    # booster parameters
    learning_rate: 0.05
    
    # Tree parameters
    max_depth: 10
    min_child_weight: 1.0
    gamma: 0.0
    max_delta_step: 0
    subsample: 1.0

    colsample_bytree:  1
    colsample_bylevel: 1
    colsample_bynode:  1
    
    reg_lambda: 2.0        # L2 regularization
    reg_alpha: 0.0         # L1 regularization
    
    scale_pos_weight: 1

    # learning task parameters
    objective: 'binary:logistic'     #
    eval_metric: ['auc', 'logloss']  # for evaluation


# Graph net
gnet3_param:
  train: 'graph'
  predict: 'torch_graph'
  label:  'G-DEC-add'

  # Convolution model
  conv_type: 'DEC'           # 'NN', SAGE', 'GINE', 'DEC', spline', 'SG', 'GAT'
  conv_aggr:   'add'
  global_pool: 'max'
  
  epochs: 150
  batch_size: 64
  learning_rate: 0.003
  weight_decay:  0.00005     # L2-regularization

  # Scheduler
  step_size: 200
  gamma: 0.1

  device: 'auto'            # alternative 'cpu:0', 'cuda:0'
  num_workers: 1

# Graph net
gnet0_param:
  train:   'graph'
  predict: 'torch_graph'
  label:  'G-NN'

  # Convolution model
  conv_type:   'NN'         # 'NN', SAGE', 'GINE', 'DEC', spline', 'SG', 'GAT'
  conv_aggr:   'max'
  global_pool: 'max'

  epochs: 150
  batch_size: 64
  learning_rate: 0.003
  weight_decay:  0.00005     # L2-regularization

  # Scheduler
  step_size: 200
  gamma: 0.1

  device: 'auto'            # alternative 'cpu:0', 'cuda:0'
  num_workers: 1

# Graph net
gnet1_param:
  train: 'graph'
  predict: 'torch_graph'
  label:  'G-SUP'

  # Convolution model
  conv_type: 'SUP'           # 'NN', SAGE', 'GINE', 'DEC', spline', 'SG', 'GAT'
  conv_aggr:   'add'
  global_pool: 'max'
  
  epochs: 20
  batch_size: 64
  learning_rate: 0.003
  weight_decay:  0.00005     # L2-regularization

  # Scheduler
  step_size: 200
  gamma: 0.1

  device: 'auto'            # alternative 'cpu:0', 'cuda:0'
  num_workers: 1

# Graph net
gnet2_param:
  train: 'graph'
  predict: 'torch_graph'
  label:  'G-EC-add'

  # Convolution model
  conv_type: 'EC'            # 'NN', SAGE', 'GINE', 'DEC', spline', 'SG', 'GAT'
  conv_aggr:   'add'
  global_pool: 'max'
  
  epochs: 150
  batch_size: 64
  learning_rate: 0.003
  weight_decay:  0.00005      # L2-regularization

  # Scheduler
  step_size: 200
  gamma: 0.1

  device: 'auto'             # alternative 'cpu:0', 'cuda:0'
  num_workers: 1


# Factorized Likelihood Ratio
flr0_param:
  train: 'flr'
  predict: 'flr'
  label: 'FLR'
  
  nbins: 60
  qmin:  0.5 # in [0,100]
  qmax: 99.5 # in [0,100]


# XGBoost
# https://xgboost.readthedocs.io/en/latest/parameter.html
xgb0_param:
  train: 'xgb'
  predict: 'xgb'
  label: 'XGB'
  
  # general parameters
  booster: 'gbtree'       # 'gbtree' (default), 'dart' (dropout boosting)
  num_boost_round: 300    # number of epochs (equal to the number of trees!)
  tree_method: 'auto'     # 'auto', 'hist' (CPU), 'gpu_hist' (GPU)

  # booster parameters
  learning_rate: 0.15
  
  # Tree parameters
  max_depth: 15
  min_child_weight: 1.0
  gamma: 0.0
  max_delta_step: 0
  subsample: 1.0

  colsample_bytree:  1
  colsample_bylevel: 1
  colsample_bynode:  1
  
  reg_lambda: 2.0       # L2 regularization
  reg_alpha: 0.0        # L1 regularization
  
  scale_pos_weight: 1

  # learning task parameters
  objective: 'binary:logistic'     #
  eval_metric: ['auc', 'logloss']  # for evaluation


# Deep MLP
dmlp_param:
  train: 'dmlp'
  predict: 'torch_generic'
  label: 'DMLP'
  
  lossfunc: 'cross_entropy' # cross_entropy, focal_entropy
  gamma: 2                  # focal_entropy exponent
  optimizer: 'AdamW'
  noise_reg: 0.0            # Noise regularization
  
  epochs: 30
  batch_size: 196
  learning_rate: 0.003
  weight_decay: 0.00001     # L2-regularization
  
  mlp_dim: [64, 64, 64]     # hidden layer dimensions
  batch_norm: True    

  device: 'auto'            # alternative 'cpu:0', 'cuda:0'
  num_workers: 1


# Logistic Regression (convex model = global optimum guarantee)
lgr0_param:
  train: 'lgr'
  predict: 'torch_generic'
  label: 'LGR'
  
  lossfunc: 'cross_entropy' # cross_entropy, focal_entropy
  gamma: 2                  # focal_entropy exponent
  optimizer: 'AdamW'
  noise_reg: 0.0            # Noise regularization

  epochs: 30
  batch_size: 196
  learning_rate: 0.003
  weight_decay: 0.00001       # L2-regularization

  device: 'auto'            # alternative 'cpu:0', 'cuda:0'
  num_workers: 1


# Convolutional Neural Net
cnn0_param:
  train: 'cnn'
  predict: 'torch_image'
  label: 'CNN'

  lossfunc: 'cross_entropy' # cross_entropy, focal_entropy
  gamma: 2                  # focal_entropy exponent
  optimizer: 'AdamW'
  noise_reg: 0.0            # Noise regularization
  
  dropout_cnn: 0.25
  dropout_mlp: 0.5
  mlp_dim: 64
  
  epochs: 100
  batch_size: 128
  learning_rate: 0.001
  weight_decay: 0.00        # L2-regularization
  device: 'auto'            # alternative 'cpu:0', 'cuda:0'
  num_workers: 1


# XTX (hyperbinned) classifier
xtx0_param:
  train: 'xtx'
  predict: 'xtx'
  label:  'XTX'

  lossfunc: 'cross_entropy' # cross_entropy, focal_entropy
  gamma: 2                  # focal_entropy exponent parameter
  optimizer: 'AdamW'        # Adam, AdamW
  noise_reg: 0.0            # Noise regularization

  num_units: 2
  neurons:  20
  dropout:  0.5
  
  epochs:  100
  batch_size:  196
  learning_rate: 0.005
  weight_decay: 0.01        # L2-regularization
  device: 'auto'            # alternative 'cpu:0', 'cuda:0'
  num_workers: 1


# Deep MAXOUT network
dmx0_param:
  train: 'dmax'
  predict: 'torch_generic'
  label:  'DMAX'
  
  lossfunc: 'cross_entropy' # cross_entropy, focal_entropy
  gamma: 2                  # focal_entropy exponent parameter
  optimizer: 'AdamW'        # Adam, AdamW
  noise_reg: 0.0            # Noise regularization

  num_units: 15
  neurons:  50
  dropout:  0.4
  
  epochs:  100
  batch_size: 128
  learning_rate: 0.005
  weight_decay: 0.00001       # L2-regularization
  device: 'auto'            # alternative 'cpu:0', 'cuda:0'
  num_workers: 1


# Deep Normalizing Flow
dbnf0_param:
  train: 'flow'
  predict: 'torch_flow'
  label:  'DBNF'
  
  # Gradient descent
  epochs: 100
  batch_size: 512
  optimizer: 'Adam'
  learning_rate: 0.05
  weight_decay: 0.0         # L2-regularization
  noise_reg: 0.001          # Noise regularization
  polyak: 0.998
  clip_norm: 0.1
  
  # Learning rate reduction on plateau
  factor:  0.1
  patience: 20
  cooldown: 10
  min_lr: 0.0005
  early_stopping: 100

  # Model structure
  flows: 10                 # number of flow blocks
  layers: 0                 # intermediate layers in a flow block
  hidden_dim: 10            # 
  residual: 'gated'         # choises 'none', 'normal', 'gated'
  perm: 'rand'              # flow permutation: choises 'none', 'flip', 'rand'
  
  modelname: 'null'
  start_epoch: 0
  tensorboard: 'tensorboard'
  device: 'auto'            # alternative 'cpu:0', 'cuda:0'
  num_workers: 4

