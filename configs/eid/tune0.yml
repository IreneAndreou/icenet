# Electron ID tune0.yml
#
# General comments:
#
# - Keep the minibatch size small enough, otherwise weak convergence to optima with some models
# - Other imputation methods than 'constant' not natively compatible with re-weighting (-> biased)
rootname: 'eid'

MAXEVENTS: 1000000               # Maximum total number of events (per root file) to process

eval_reweight: False             # Event-by-event re-weighting used in the evaluation phase


# ---------------------------------------------------------
# ** Activate models here **
# Give all models some unique identifier and label
active_models:

#  - dmlp
#  - cnn0
  - gnet0
  - gnet1
#  - gnet2
#  - gnet3
#  - gnet4

#  - flr0
  - xgb0
  - lgr0
#  - xtx0
#  - dmx0
#  - dbnf0
  - gxgb0
# ---------------------------------------------------------


# Batched "deep" training
batch_train_param:
  blocksize: 1000000            # Maximum number of events simultaneously in RAM
  epochs: 50                   # Number of global epochs (1 epoch = one iteration over the full input dataset)
  local_epochs: 1               # Number of local epochs per algorithm. If 0, then use the custom value per model.
  num_cpu: null                 # Set null for auto, or an integer for manual.


rngseed: 123456                 # Fixed seed for training data mixing
inputvar: 'CMSSW_MVA_ID_ORIG'   # Input variables, implemented under mvavars.py

targetfunc: 'target_e'          # Training target,    implemented under mctargets.py
filterfunc: 'filter_no_egamma'  # Training filtering, implemented under mcfilter.py
cutfunc:    'cut_standard'      # Basic cuts,         implemented under cuts.py

xcorr_flow: True                # Full N-point correlations computed between cuts

frac: 0.9                       # Train/validate/test split fraction
varnorm: 'zscore'               # Variable normalization: 'zscore', 'madscore', 'none'
varnorm_tensor: 'zscore'        # Tensor variable normalization
#varnorm_graph: 'none'          # Not implemented yet


# ** Graph object construction **
graph_param:
  global_on: True
  coord: 'pxpypze'              # 'ptetaphim', 'pxpypze'


# ** Image tensor object construction **
image_param:

  # See the corresponding construction under common.py
  channels: 2                 # 1,2,...

  # bin-edges
  

  eta_bins: [-1.5 , -1.44915254, -1.39830508, -1.34745763, -1.29661017,
       -1.24576271, -1.19491525, -1.1440678 , -1.09322034, -1.04237288,
       -0.99152542, -0.94067797, -0.88983051, -0.83898305, -0.78813559,
       -0.73728814, -0.68644068, -0.63559322, -0.58474576, -0.53389831,
       -0.48305085, -0.43220339, -0.38135593, -0.33050847, -0.27966102,
       -0.22881356, -0.1779661 , -0.12711864, -0.07627119, -0.02542373,
        0.02542373,  0.07627119,  0.12711864,  0.1779661 ,  0.22881356,
        0.27966102,  0.33050847,  0.38135593,  0.43220339,  0.48305085,
        0.53389831,  0.58474576,  0.63559322,  0.68644068,  0.73728814,
        0.78813559,  0.83898305,  0.88983051,  0.94067797,  0.99152542,
        1.04237288,  1.09322034,  1.1440678 ,  1.19491525,  1.24576271,
        1.29661017,  1.34745763,  1.39830508,  1.44915254,  1.5       ]

  #eta_bins: [-1.5, -1.4, -1.3, -1.2, -1.1, -1. , -0.9, -0.8, -0.7, -0.6, -0.5,
  #     -0.4, -0.3, -0.2, -0.1,  0. ,  0.1,  0.2,  0.3,  0.4,  0.5,  0.6,
  #      0.7,  0.8,  0.9,  1. ,  1.1,  1.2,  1.3,  1.4,  1.5]
  
  #phi_bins: [-6,-5,-4,-3,-2,-1,0,1,2,3,4,5,6]
  
  phi_bins: [-3.14159, -3.03509542, -2.92860085, -2.82210627, -2.71561169,
       -2.60911712, -2.50262254, -2.39612797, -2.28963339, -2.18313881,
       -2.07664424, -1.97014966, -1.86365508, -1.75716051, -1.65066593,
       -1.54417136, -1.43767678, -1.3311822 , -1.22468763, -1.11819305,
       -1.01169847, -0.9052039 , -0.79870932, -0.69221475, -0.58572017,
       -0.47922559, -0.37273102, -0.26623644, -0.15974186, -0.05324729,
        0.05324729,  0.15974186,  0.26623644,  0.37273102,  0.47922559,
        0.58572017,  0.69221475,  0.79870932,  0.9052039 ,  1.01169847,
        1.11819305,  1.22468763,  1.3311822 ,  1.43767678,  1.54417136,
        1.65066593,  1.75716051,  1.86365508,  1.97014966,  2.07664424,
        2.18313881,  2.28963339,  2.39612797,  2.50262254,  2.60911712,
        2.71561169,  2.82210627,  2.92860085,  3.03509542,  3.14159   ]
  
  #phi_bins: [-3.14159   , -2.93215067, -2.72271133, -2.513272  , -2.30383267,
  #     -2.09439333, -1.884954  , -1.67551467, -1.46607533, -1.256636  ,
  #     -1.04719667, -0.83775733, -0.628318  , -0.41887867, -0.20943933,
  #      0.        ,  0.20943933,  0.41887867,  0.628318  ,  0.83775733,
  #      1.04719667,  1.256636  ,  1.46607533,  1.67551467,  1.884954  ,
  #      2.09439333,  2.30383267,  2.513272  ,  2.72271133,  2.93215067,
  #      3.14159   ]


## Reweighting setup in the training phase
reweight_param:

  maxevents: 1000000            # Maximum number of events for the PDF construction
  equal_frac: true              # Equalize integrated class fractions
  # ---------------------
  
  differential_reweight: false
  reference_class: 0            # Reference class: 0 = (background), 1 = (signal), 2 = (another class) ..., 
  # Note about asymmetry: Keep the reference class as 0 == background, because signal
  # MC has much longer tail over pt. Thus, it is not possible to re-weight background
  # to signal very easily, but it is possible to re-weight signal to background.
  # This is seen by inspecting the re-weighted distributions with small and large statistics.

  var_A:  'trk_pt'
  var_B:  'trk_eta'

  reference_class: 0           # Reference class: -1 = (turned off), 0 = (background), 1 = (signal), 
  # Note about asymmetry: Keep the reference class as 0 == background, because signal
  # MC has much longer tail over pt. Thus, it is not possible to re-weight background
  # to signal very easily, but it is possible to re-weight signal to background.
  # This is seen by inspecting the re-weighted distributions with small and large statistics.

  bins_A:  [0.0, 50.0, 200]    # Make sure the bounds cover the phase space
  binmode_A:  'linear'         # 'log10' or 'linear' binning
  
  bins_B: [-3.1, 3.1, 100]     #
  binmode_B: 'linear'
  
  # ! Variable, and binning min/max boundaries are both transformed !
  transform_A: 'log10'         # 'log10', 'sqrt', 'square', null
  transform_B: null
  
  max_reg: 1000.0              # Maximum weight cut-off regularization


# Imputation
imputation_param:
  active: true                 # True / False
  var: 'CMSSW_MVA_ID_ORIG'     # Array of variables to be imputated
  algorithm: 'constant'        # Algorithm type: 'constant', iterative' (vector), knn' (vector), 'mean' (scalar), 'median' (scalar)
  fill_value: 0                # For constant imputation
  knn_k: 8                     # Number of nearest neighbours considered
  values: [-999, -666, -10]    # Special values which indicate the need for imputation


## Outlier protection in the training phase
outlier_param:
  algo: 'truncate'   # algorithm: 'truncate', 'none'
  qmin: 0.01         # in [0,100] 
  qmax: 99.9         # in [0,100]



## Pure plotting setup
plot_param:
  basic_on:    false
  contours_on: false

  ## Binned ROC plots can be 1D or 2D;
  # use a running number __0, __1, ... as an identifier for new plots
  plot_ROC_binned__0:
    var:   ['trk_pt']
    edges: [0.5, 0.7, 1.0, 1.25, 1.5, 1.75, 2.5, 4.0, 10, 10000]

  plot_ROC_binned__1:
    var:   ['trk_eta', 'trk_pt']
    edges: [[-2.5, -1.5, -1.15, -0.75, 0.0, 0.75, 1.15, 1.5, 2.5],
            [0.5, 0.7, 1.0, 1.25, 1.5, 1.75, 2.5, 4.0, 10, 10000]]


# ========================================================================
## Hyperparameter raytune setup
# ========================================================================


raytune_param:
  active:         [] # Add model identifiers here
  num_samples:    [10]
  max_num_epochs: [10]


raytune_setup:

  trial0:
    search_algo: 'HyperOpt'

    search_metric:
      metric: 'AUC'
      mode: 'max'

    param:
      cdim:
        type: "tune.choice"
        value: [32, 64, 96, 128, 196, 256, 512, 1024]

      global_pool:
        type: "tune.choice"
        value: ['mean', 'max', 'add', 'mean']

      batch_size:
        type: "tune.choice"
        value: [16, 32, 64, 128]

      learning_rate:
        type: "tune.loguniform"
        value: [1.0E-5, 1.0E-1] # Careful with with .0E (otherwise -> string)

    #
    #batch_size:
    #  type: "tune.choice"
    #  value: [32, 64, 128, 256]
    #

    # Add more steers here
    # ...



# ========================================================================
## Classifier setup
# ========================================================================

# Dual graph + XGB network
gxgb0_param:

  train:   'graph_xgb'
  predict: 'graph_xgb'
  label:   'DGXGB'

  # ** GRAPH NETWORK **
  graph:
    train:   'graph'
    predict: 'torch_graph'
    label:   'GXGB-G'
    raytune:  null

    # Convolution model
    conv_type:   'SUP'           # 'NN', SAGE', 'GINE', 'DEC', spline', 'SG', 'GAT'

    # Model
    model_param:
      conv_aggr:   'add'
      global_pool: 'mean'
      cdim: 96                   # Convolution output dimension
    
    # Optimization
    opt_param:  
      epochs: 50
      batch_size: 64
      learning_rate: 0.003
      weight_decay:  0.00005     # L2-regularization

    # Scheduler
    scheduler_param:
      step_size: 200
      gamma: 0.1
    
    device: 'auto'               # alternative 'cpu:0', 'cuda:0'
    num_workers: 4
    
    # Read/Write of epochs
    savemode: 'all'              # 'all', 'latest'
    readmode: -1                 # -1 is the last saved epoch

  # ** XGBOOST **
  xgb:
    train:   'xgb'
    predict: 'xgb'
    label:   'GXGB-XGB'
    raytune:  null

    # general parameters
    booster: 'gbtree'       # 'gbtree' (default), 'dart' (dropout boosting)
    num_boost_round: 300    # number of epochs (equal to the number of trees!)
    tree_method: 'auto'     # 'auto', 'hist' (CPU), 'gpu_hist' (GPU)
    
    # booster parameters
    learning_rate: 0.3
    gamma: 0.0
    max_depth: 10
    min_child_weight: 1.0
    max_delta_step: 1
    subsample: 1

    colsample_bytree:  1
    colsample_bylevel: 1
    colsample_bynode:  1
    
    reg_lambda: 2.0        # L2 regularization
    reg_alpha: 0.0         # L1 regularization
    
    scale_pos_weight: 1

    # learning task parameters
    objective: 'binary:logistic'     #
    eval_metric: ['auc', 'logloss']  # for evaluation

    # Read/Write of epochs
    savemode: 'all'                  # 'all', 'latest'
    readmode: -1                     # -1 is the last saved epoch


# Graph net
gnet0_param:
  train:    'graph'
  predict:  'torch_graph'
  label:    'G-NN'
  raytune:  'trial0'

  conv_type:   'NN'           # 'NN', SAGE', 'GINE', 'DEC', spline', 'SG', 'GAT'

  # Model
  model_param:
    conv_aggr:   'max'
    global_pool: 'mean'
    cdim: 196                 # Convolution output dimension

  # Optimization
  opt_param:
    epochs: 50
    batch_size: 64
    learning_rate: 0.003
    weight_decay:  0.00005      # L2-regularization

  # Scheduler
  scheduler_param:
    step_size: 200
    gamma: 0.1

  device: 'auto'              # alternative 'cpu:0', 'cuda:0'
  num_workers: 4

  # Read/Write of epochs
  savemode: 'all'             # 'all', 'latest'
  readmode: -1                # 'name', epoch number, or -1 uses the last saved epoch


# Graph net
gnet1_param:
  train:   'graph'
  predict: 'torch_graph'
  label:   'G-SUP'
  raytune:  null

  # Convolution model
  conv_type: 'SUP'              # 'NN', SAGE', 'GINE', 'DEC', spline', 'SG', 'GAT'

  # Model
  model_param:
    conv_aggr:   'add'
    global_pool: 'mean'
    cdim: 96                   # Convolution output dimension

  # Optimization
  opt_param:
    epochs: 50
    batch_size: 64
    learning_rate: 0.003
    weight_decay:  0.00005     # L2-regularization

  # Scheduler
  scheduler_param:
    step_size: 200
    gamma: 0.1

  device: 'auto'               # alternative 'cpu:0', 'cuda:0'
  num_workers: 4

  # Read/Write of epochs
  savemode: 'all'              # 'all', 'latest'
  readmode: -1                 # -1 is the last saved epoch


# Graph net
gnet2_param:
  train:   'graph'
  predict: 'torch_graph'
  label:   'G-EC-add'
  raytune:  null

  # Convolution model
  conv_type: 'EC'            # 'NN', SAGE', 'GINE', 'DEC', spline', 'SG', 'GAT'

  # Model
  model_param:
    conv_aggr:   'add'
    global_pool: 'mean'
    cdim: 96                   # Convolution output dimension
  
  # Optimization
  opt_param:
    epochs: 50
    batch_size: 64
    learning_rate: 0.003
    weight_decay:  0.00005      # L2-regularization

  # Scheduler
  scheduler_param:
    step_size: 200
    gamma: 0.1

  device: 'auto'             # alternative 'cpu:0', 'cuda:0'
  num_workers: 4

  # Read/Write of epochs
  savemode: 'all'              # 'all', 'latest'
  readmode: -1                 # -1 is the last saved epoch


# Graph net
gnet3_param:
  train:   'graph'
  predict: 'torch_graph'
  label:   'G-DEC-add'
  raytune:  'trial0'

  # Convolution model
  conv_type: 'DEC'           # 'NN', SAGE', 'GINE', 'DEC', spline', 'SG', 'GAT'

  # Model
  model_param:
    conv_aggr:   'add'
    global_pool: 'mean'
    cdim: 96                   # Convolution output dimension
  
  # Optimization
  opt_param:
    epochs: 50
    batch_size: 64
    learning_rate: 0.003
    weight_decay:  0.00005     # L2-regularization

  # Scheduler
  scheduler_param:
    step_size: 200
    gamma: 0.1

  device: 'auto'             # alternative 'cpu:0', 'cuda:0'
  num_workers: 4

  # Read/Write of epochs
  savemode: 'all'              # 'all', 'latest'
  readmode: -1                 # -1 is the last saved epoch


# Graph net
gnet4_param:
  train:   'graph'
  predict: 'torch_graph'
  label:   'G-GAT'
  raytune:  null

  # Convolution model
  conv_type:   'GAT'         # 'NN', SAGE', 'GINE', 'DEC', spline', 'SG', 'GAT'

  # Model
  model_param:
    conv_aggr:   'max'
    global_pool: 'mean'
    cdim: 64                   # Convolution output dimension

  # Optimization
  opt_param:
    epochs: 50
    batch_size: 64
    learning_rate: 0.003
    weight_decay:  0.00005     # L2-regularization

  # Scheduler
  scheduler_param:
    step_size: 200
    gamma: 0.1

  device: 'auto'            # alternative 'cpu:0', 'cuda:0'
  num_workers: 4

  # Read/Write of epochs
  savemode: 'all'              # 'all', 'latest'
  readmode: -1                 # -1 is the last saved epoch


# Factorized Likelihood Ratio
flr0_param:
  train:   'flr'
  predict: 'flr'
  label:   'FLR'
  raytune:  null

  nbins: 60
  qmin:  0.5 # in [0,100]
  qmax: 99.5 # in [0,100]


# XGBoost
# https://xgboost.readthedocs.io/en/latest/parameter.html
xgb0_param:
  train:   'xgb'
  predict: 'xgb'
  label:   'XGB'
  raytune:  null

  # general parameters
  booster: 'gbtree'       # 'gbtree' (default), 'dart' (dropout boosting)
  num_boost_round: 300    # number of epochs (equal to the number of trees!)
  tree_method: 'auto'     # 'auto', 'hist' (CPU), 'gpu_hist' (GPU)


  # booster parameters
  learning_rate: 0.3
  gamma: 0.0
  max_depth: 10
  min_child_weight: 1.0
  max_delta_step: 1
  subsample: 1

  colsample_bytree:  1
  colsample_bylevel: 1
  colsample_bynode:  1
  
  reg_lambda: 2.0       # L2 regularization
  reg_alpha: 0.0        # L1 regularization
  
  scale_pos_weight: 1

  # learning task parameters
  objective: 'binary:logistic'     #
  eval_metric: ['auc', 'logloss']  # for evaluation

  # Read/Write of epochs
  savemode: 'all'              # 'all', 'latest'
  readmode: -1                 # -1 is the last saved epoch


# Deep MLP
dmlp_param:
  train:   'dmlp'
  predict: 'torch_generic'
  label:   'DMLP'
  raytune:  null

  # Optimization
  opt_param:

    lossfunc: 'cross_entropy' # cross_entropy, focal_entropy
    gamma: 2                  # focal_entropy exponent
    optimizer: 'AdamW'
    noise_reg: 0.0            # Noise regularization
    
    epochs: 50
    batch_size: 196
    learning_rate: 0.003
    weight_decay: 0.00001     # L2-regularization
  
  # Model
  model_param:
    mlp_dim: [64, 64, 64]     # hidden layer dimensions
    batch_norm: True    


  device: 'auto'            # alternative 'cpu:0', 'cuda:0'
  num_workers: 4

  # Read/Write of epochs
  savemode: 'all'              # 'all', 'latest'
  readmode: -1                 # -1 is the last saved epoch


# Logistic Regression (convex model = global optimum guarantee)
lgr0_param:
  train:   'lgr'
  predict: 'torch_generic'
  label:   'LGR'
  raytune:  null

  opt_param:
    lossfunc: 'cross_entropy' # cross_entropy, focal_entropy
    gamma: 2                  # focal_entropy exponent
    optimizer: 'AdamW'
    noise_reg: 0.0            # Noise regularization

    epochs: 50
    batch_size: 196
    learning_rate: 0.003
    weight_decay: 0.00001       # L2-regularization

  device: 'auto'            # alternative 'cpu:0', 'cuda:0'
  num_workers: 4

  # Read/Write of epochs
  savemode: 'all'              # 'all', 'latest'
  readmode: -1                 # -1 is the last saved epoch


# Convolutional Neural Net
cnn0_param:
  train:   'cnn'
  predict: 'torch_image'
  label:   'CNN'
  raytune:  null

  # Optimization
  opt_param:  
    lossfunc: 'cross_entropy' # cross_entropy, focal_entropy
    gamma: 2                  # focal_entropy exponent
    optimizer: 'AdamW'
    noise_reg: 0.0            # Noise regularization
  
    epochs: 50
    batch_size: 128
    learning_rate: 0.001
    weight_decay: 0.00        # L2-regularization
  
  # Model
  model_param:
    dropout_cnn: 0.25
    dropout_mlp: 0.5
    mlp_dim: 64
    
  device: 'auto'            # alternative 'cpu:0', 'cuda:0'
  num_workers: 4

  # Read/Write of epochs
  savemode: 'all'              # 'all', 'latest'
  readmode: -1                 # -1 is the last saved epoch


# XTX (hyperbinned) classifier
xtx0_param:
  train:   'xtx'
  predict: 'xtx'
  label:   'XTX'
  raytune:  null

  # Optimization
  opt_param:
    lossfunc: 'cross_entropy' # cross_entropy, focal_entropy
    gamma: 2                  # focal_entropy exponent parameter
    optimizer: 'AdamW'        # Adam, AdamW
    noise_reg: 0.0            # Noise regularization
    
    epochs: 50
    batch_size:  196
    learning_rate: 0.005
    weight_decay: 0.01        # L2-regularization

  # Model param
  model_param:
    num_units: 2
    neurons:  20
    dropout:  0.5

  device: 'auto'            # alternative 'cpu:0', 'cuda:0'
  num_workers: 4

  # Read/Write of epochs
  savemode: 'all'              # 'all', 'latest'
  readmode: -1                 # -1 is the last saved epoch


# Deep MAXOUT network
dmx0_param:
  train:   'dmax'
  predict: 'torch_generic'
  label:   'DMAX'
  raytune:  null

  # Optimization
  opt_param:
    lossfunc: 'cross_entropy' # cross_entropy, focal_entropy
    gamma: 2                  # focal_entropy exponent parameter
    optimizer: 'AdamW'        # Adam, AdamW
    noise_reg: 0.0            # Noise regularization

    epochs: 50
    batch_size: 128
    learning_rate: 0.005
    weight_decay: 0.00001       # L2-regularization

  # Model parameter
  model_param:  
    num_units: 15
    neurons:  50
    dropout:  0.4
    
  device: 'auto'            # alternative 'cpu:0', 'cuda:0'
  num_workers: 4

  # Read/Write of epochs
  savemode: 'all'              # 'all', 'latest'
  readmode: -1                 # -1 is the last saved epoch


# Deep Normalizing Flow
dbnf0_param:
  train:   'flow'
  predict: 'torch_flow'
  label:   'DBNF'
  raytune:  null

  # Gradient descent
  opt_param:
    epochs: 50
    batch_size: 512
    optimizer: 'Adam'
    learning_rate: 0.05
    weight_decay: 0.0         # L2-regularization
    noise_reg: 0.001          # Noise regularization
    polyak: 0.998
    clip_norm: 0.1
    start_epoch: 0

  # Learning rate reduction on plateau
  scheduler_param:  
    factor:  0.1
    patience: 20
    cooldown: 10
    min_lr: 0.0005
    early_stopping: 100

  # Model structure
  model_param:  
    flows: 10                 # number of flow blocks
    layers: 0                 # intermediate layers in a flow block
    hidden_dim: 10            # 
    residual: 'gated'         # choises 'none', 'normal', 'gated'
    perm: 'rand'              # flow permutation: choises 'none', 'flip', 'rand'
  
  modelname: 'null'
  tensorboard: 'tensorboard'
  device: 'auto'            # alternative 'cpu:0', 'cuda:0'
  num_workers: 4

  # Read/Write of epochs
  savemode: 'all'              # 'all', 'latest'
  readmode: -1                 # -1 is the last saved epoch

