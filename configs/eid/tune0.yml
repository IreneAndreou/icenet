# Electron ID tune0.yml
#
# General comments:
#
# - Keep the minibatch size small enough, otherwise weak convergence to optima
# - Other imputation methods than 'constant' not natively compatible with re-weighting (-> biased)
#

MAXEVENTS: 200000

rngseed: 123456                # Fixed seed for training data mixing
inputvar: 'CMSSW_MVA_ID_ORIG'  # Input variables, implemented under mvavars.py

targetfunc: 'target_e'         # Training target,    implemented under mctargets.py
filterfunc: 'filter_no_egamma' # Training filtering, implemented under mcfilter.py

cutfunc: 'cut_standard'        # Basic cuts,              implemented under cuts.py
frac: 0.9                      # Train/validate/test split fraction
varnorm: 'zscore'              # Variable normalization: 'zscore', 'madscore', 'none'


# Imputation
imputation_param:
  active: true                 # True / False
  var: 'CMSSW_MVA_ID_ORIG'     # Array of variables to be imputated
  algorithm: 'constant'        # Algorithm type: 'constant', iterative' (vector), knn' (vector), 'mean' (scalar), 'median' (scalar)
  fill_value: 0                # For constant imputation
  knn_k: 8                     # Number of nearest neighbours considered
  values: [-999, -666, -10]    # Special values which indicate the need for imputation


## Outlier protection in the training phase
outlier_param:
  algo: 'truncate'   # algorithm: 'truncate', 'none'
  qmin: 0.01         # in [0,100] 
  qmax: 99.9         # in [0,100]


## Reweighting setup in the training phase
reweight_param:
  mode: 'background'           # 'none', 'signal', 'background'
  algo: '2D'
  bins_pt:  [0.0, 300.0, 1000] # Make sure the bounds cover the phase space
  bins_eta: [-3.1, 3.1,   100] # 
  max_reg: 50.0                # maxweight regularization


## Pure plotting setup
plot_param:
  basic_on:    false
  contours_on: false
  
  # (eta,pt)-binned plots
  pt_edges:  [0, 0.75, 1.0, 1.25, 1.5, 1.75, 2.5, 4.0, 10, 10000]
  eta_edges: [-2.5, -1.5, -0.75, 0.0, 0.75, 1.5, 2.5]


# ========================================================================
## Classifier setup
# ========================================================================

# Factorized Likelihood Ratio
flr_param:
  active: true
  label: 'FLR'
  
  nbins: 60
  qmin:  0.5 # in [0,100]
  qmax: 99.5 # in [0,100]


# XGBoost
# https://xgboost.readthedocs.io/en/latest/parameter.html
xgb_param:
  active: true
  label: 'XGB'
  
  # general parameters
  booster: 'gbtree'       # 'gbtree' (default), 'dart' (dropout boosting)
  num_boost_round: 300    # Number of epochs
  tree_method: 'auto'     # 'auto', 'hist' (CPU), 'gpu_hist' (GPU)

  # booster parameters
  learning_rate: 0.15
  
  max_depth: 15
  min_child_weight: 1.0
  gamma: 0.0
  max_delta_step: 0
  subsample: 1.0

  colsample_bytree:  1
  colsample_bylevel: 1
  colsample_bynode:  1
  
  reg_lambda: 2.0       # L2 regularization
  reg_alpha: 0.0        # L1 regularization
  
  scale_pos_weight: 1

  # learning task parameters
  objective: 'binary:logistic'     #
  eval_metric: ['auc', 'logloss']  # for evaluation


# Logistic Regression (convex algorithm = global optimum guarantee)
mlgr_param:
  active: true
  label:  'MLGR'
  
  lossfunc: 'cross_entropy' # cross_entropy, focal_entropy
  gamma: 2                  # focal_entropy exponent
  optimizer: 'AdamW'
  noise_reg: 0.0            # Noise regularization

  epochs: 30
  batch_size: 196
  learning_rate: 0.003
  weight_decay: 0.01        # L2-regularization
  device: 'auto'            # alternative 'cpu:0', 'cuda:0'
  num_workers: 1


# Convolutional Neural Net
cnn_param:
  active: True
  label:  'CNN'

  lossfunc: 'cross_entropy' # cross_entropy, focal_entropy
  gamma: 2                  # focal_entropy exponent
  optimizer: 'AdamW'
  noise_reg: 0.0            # Noise regularization

  dropout_cnn: 0.1
  dropout_mlp: 0.5
  mlp_dim: 64
  
  epochs: 100
  batch_size: 128
  learning_rate: 0.001
  weight_decay: 0.00        # L2-regularization
  device: 'auto'            # alternative 'cpu:0', 'cuda:0'
  num_workers: 1


# CNN + MAXOUT
cdmx_param:
  active: False
  label:  'CDMX'
  
  lossfunc: 'cross_entropy' # cross_entropy, focal_entropy
  gamma: 2                  # focal_entropy exponent
  optimizer: 'AdamW'
  noise_reg: 0.0            # Noise regularization
  
  # CNN param
  dropout_cnn: 0.4
  
  # MAXOUT param
  num_units: 15
  neurons:  50
  dropout:  0.1

  epochs: 60
  batch_size: 128
  learning_rate: 0.001
  weight_decay: 0.00        # L2-regularization
  device: 'auto'            # alternative 'cpu:0', 'cuda:0'
  num_workers: 1


# XTX (hyperbinned) classifier
xtx_param:
  active: false
  label:  'XTX'

  lossfunc: 'cross_entropy' # cross_entropy, focal_entropy
  gamma: 2                  # focal_entropy exponent parameter
  optimizer: 'AdamW'        # Adam, AdamW
  noise_reg: 0.0            # Noise regularization

  num_units: 2
  neurons:  20
  dropout:  0.5
  
  epochs:  100
  batch_size:  196
  learning_rate: 0.005
  weight_decay: 0.01        # L2-regularization
  device: 'auto'            # alternative 'cpu:0', 'cuda:0'
  num_workers: 1


# Deep MAXOUT network
dmax_param:
  active: true
  label:  'DMAX'
  
  lossfunc: 'cross_entropy' # cross_entropy, focal_entropy
  gamma: 2                  # focal_entropy exponent parameter
  optimizer: 'AdamW'        # Adam, AdamW
  noise_reg: 0.0            # Noise regularization

  num_units: 15
  neurons:  50
  dropout:  0.1
  
  epochs:  200
  batch_size: 256
  learning_rate: 0.005
  weight_decay: 0.01        # L2-regularization
  device: 'auto'            # alternative 'cpu:0', 'cuda:0'
  num_workers: 1

# Deep Normalizing Flow
dbnf_param:
  active: false
  label:  'DBNF'
  
  # Gradient descent
  epochs: 1000
  batch_size: 512
  optimizer: 'Adam'
  learning_rate: 0.05
  weight_decay: 0.0         # L2-regularization
  noise_reg: 0.001          # Noise regularization
  polyak: 0.998
  clip_norm: 0.1
  
  # Learning rate reduction on plateau
  factor:  0.1
  patience: 20
  cooldown: 10
  min_lr: 0.0005
  early_stopping: 100

  # Model structure
  flows: 10                 # number of flow blocks
  layers: 0                 # intermediate layers in a flow block
  hidden_dim: 10            # 
  residual: 'gated'         # choises 'none', 'normal', 'gated'
  perm: 'rand'              # flow permutation: choises 'none', 'flip', 'rand'
  
  modelname: 'null'
  start_epoch: 0
  tensorboard: 'tensorboard'
  device: 'auto'            # alternative 'cpu:0', 'cuda:0'
  num_workers: 4


